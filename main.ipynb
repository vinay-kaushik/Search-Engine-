{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easygui as eg\n",
    "import numpy as np\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import operator\n",
    "from nltk.stem import PorterStemmer\n",
    "import webbrowser\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def user_input(message, title):\n",
    "    text = eg.enterbox(message, title)\n",
    "    if text is None:\n",
    "        exit()\n",
    "    return text\n",
    "\n",
    "def display_results_gui (webLinks, page_order, PreprocessedQuery,numberOfResults):\n",
    "    \n",
    "    # print(str(PreprocessedQuery),\"hi\")\n",
    "    message = \"Preprocessed query: \"+str(PreprocessedQuery)+\"\\nThese are the results of your query, you can double click\" \\\n",
    "                                                   \" or select and press ok on a\" \\\n",
    "                                                   \" result to open the web page in a new tab of your default browser.\" \\\n",
    "                                                   \" Press cancel to go back to the main menu.\"\n",
    "    results = []\n",
    "    for i in range(0, numberOfResults):\n",
    "        results.append(webLinks[page_order[i][0]])\n",
    "\n",
    "    return eg.choicebox(message, \"SearchEngine results\", results)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def show_links_gui(webLinks, page_order, query,numberOfResults):\n",
    "    choice = display_results_gui(webLinks, page_order, query,numberOfResults)\n",
    "    if choice is None:\n",
    "        exit()\n",
    "    else:\n",
    "        OpenUrl(choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query =  cornelia\n",
      "1 cs.uic.edu/profiles/cornelia-caragea\n",
      "2 cs.uic.edu\n",
      "3 uic.edu\n",
      "4 engineering.uic.edu/undergraduate-admissions\n",
      "5 engineering.uic.edu/support\n",
      "6 engineeringalumni.uic.edu\n",
      "7 engineering.uic.edu\n",
      "8 engineering.uic.edu/undergraduate/undergraduate-admissions\n",
      "9 cs.uic.edu/graduate/admissions\n",
      "10 cs.uic.edu/cs-events/student-opportunities\n",
      "11 cs.uic.edu/cs-events/calendar\n",
      "12 cs.uic.edu/news-stories/brent-stephens-receives-nsf-career-award\n",
      "13 cs.uic.edu/news-stories/cs-assistant-professor-zheleva-works-to-infuse-causal-inference-into-machine-learning-algorithm-design\n",
      "14 cs.uic.edu/news-stories/uic-cs-students-help-chicago-public-school-students-explore-computer-science\n",
      "15 cs.uic.edu/cs-news/all-news\n",
      "16 catalog.uic.edu/ucat/academic-calendar\n",
      "17 uic.edu/apps/departments-az/search\n",
      "18 disabilityresources.uic.edu\n",
      "19 emergency.uic.edu\n",
      "20 uic.edu/about/job-opportunities\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "def OpenUrl(url):\n",
    "    webbrowser.open_new(url)\n",
    "\n",
    "if (__name__ == \"__main__\"):\n",
    "\n",
    "    pagesCount = 4000\n",
    "    query = user_input(\"Search Query: \", \"SearchEngine\")\n",
    "    print(\"User Query = \",query)\n",
    "    # print(query)\n",
    "\n",
    "\n",
    "    # converting into lower case and splitting the words\n",
    "    query = query.split(\" \")\n",
    "    temp_q = \"\"\n",
    "    ps = PorterStemmer()\n",
    "    for word in query:\n",
    "        temp = word.lower()\n",
    "        temp = ps.stem(temp)\n",
    "        temp_q += (temp + \" \")\n",
    "\n",
    "    query = temp_q.rstrip(\" \")\n",
    "\n",
    "    with open(\"./uic_crawledLinks_\"+str(pagesCount),\"rb\") as data_file:\n",
    "        webLinks = pickle.load(data_file)\n",
    "\n",
    "    with open(\"./index2_\"+str(pagesCount),\"rb\") as outfile:\n",
    "        stream_length_title = pickle.load(outfile)\n",
    "        stream_length = pickle.load(outfile)\n",
    "        IDF_title = pickle.load(outfile)\n",
    "        IDF = pickle.load(outfile)\n",
    "        inv_index_title = pickle.load(outfile)\n",
    "        inv_index = pickle.load(outfile)\n",
    "        TF_IDF_title = pickle.load(outfile)\n",
    "        TF_IDF = pickle.load(outfile)\n",
    "        no_slashes = pickle.load(outfile)\n",
    "        len_URL = pickle.load(outfile)\n",
    "        outlink_count = pickle.load(outfile)\n",
    "        inlink_count = pickle.load(outfile)\n",
    "        url_split = pickle.load(outfile)\n",
    "\n",
    "    with open(\"./pagerank_\"+str(pagesCount),\"rb\") as outfile:\n",
    "        page_rank = pickle.load(outfile)\n",
    "\n",
    "    IDF_queue = 0\n",
    "    IDF_queue_title = 0\n",
    "    for word in query:\n",
    "        IDF_queue += IDF.get(word, 0)\n",
    "        IDF_queue_title += IDF_title.get(word, 0)\n",
    "\n",
    "    termFrequency_sum = {}\n",
    "    termFrequency_min = {}\n",
    "    termFrequency_max = {}\n",
    "    covered_terms = {}\n",
    "\n",
    "    for document, word_mapping in inv_index.items():\n",
    "        covered_terms[document] = 0\n",
    "\n",
    "    temp = query.split(\" \")\n",
    "    for word in temp:\n",
    "        for document, word_mapping in inv_index.items():\n",
    "            termFrequency_sum[document] = termFrequency_sum.get(document, 0) + word_mapping.get(word, 0)\n",
    "            if (word_mapping.get(word, 0) != 0):\n",
    "                covered_terms[document] = covered_terms.get(document) + 1\n",
    "\n",
    "            if (termFrequency_min.get(document, 0) == 0):\n",
    "                termFrequency_min[document] = word_mapping.get(word, 0)\n",
    "            else:\n",
    "                if (termFrequency_min[document] > word_mapping.get(word, float(\"Inf\"))):\n",
    "                    termFrequency_min[document] = word_mapping.get(word)\n",
    "\n",
    "            if (termFrequency_max.get(document, 0) == 0):\n",
    "                termFrequency_max[document] = word_mapping.get(word, 0)\n",
    "            else:\n",
    "                if (termFrequency_max[document] < word_mapping.get(word, 0)):\n",
    "                    termFrequency_max[document] = word_mapping.get(word, 0)\n",
    "\n",
    "    covered_term_ratio = {}\n",
    "    for document, freq in covered_terms.items():\n",
    "        covered_term_ratio[document] = freq / len(temp)\n",
    "\n",
    "    termFrequency_title_sum = {}\n",
    "    termFrequency_title_min = {}\n",
    "    termFrequency_title_max = {}\n",
    "    termFrequency_title_mean = {}\n",
    "    covered_terms_title = {}\n",
    "\n",
    "    temp = query.split(\" \")\n",
    "    for document, word_mapping in inv_index_title.items():\n",
    "        covered_terms_title[document] = 0\n",
    "    for word in temp:\n",
    "        for document, word_mapping in inv_index_title.items():\n",
    "            termFrequency_title_sum[document] = termFrequency_title_sum.get(document, 0) + word_mapping.get(word, 0)\n",
    "            if (word_mapping.get(word, 0) != 0):\n",
    "                covered_terms_title[document] = covered_terms_title.get(document) + 1\n",
    "\n",
    "            if (termFrequency_title_min.get(document, 0) == 0):\n",
    "                termFrequency_title_min[document] = word_mapping.get(word, 0)\n",
    "            else:\n",
    "                if (termFrequency_title_min[document] > word_mapping.get(word, float(\"Inf\"))):\n",
    "                    termFrequency_title_min[document] = word_mapping.get(word)\n",
    "\n",
    "            if (termFrequency_title_max.get(document, 0) == 0):\n",
    "                termFrequency_title_max[document] = word_mapping.get(word, 0)\n",
    "            else:\n",
    "                if (termFrequency_title_max[document] < word_mapping.get(word, 0)):\n",
    "                    termFrequency_title_max[document] = word_mapping.get(word, 0)\n",
    "\n",
    "\n",
    "    tf_idf_title_sum = {}\n",
    "    tf_idf_title_min = {}\n",
    "    tf_idf_title_max = {}\n",
    "    tf_idf_title_mean = {}\n",
    "    for word in temp:\n",
    "        for document, word_mapping in TF_IDF_title.items():\n",
    "            tf_idf_title_sum[document] = tf_idf_title_sum.get(document, 0) + word_mapping.get(word, 0)\n",
    "\n",
    "            if (tf_idf_title_min.get(document, 0) == 0):\n",
    "                tf_idf_title_min[document] = word_mapping.get(word, 0)\n",
    "            else:\n",
    "                if (tf_idf_title_min[document] > word_mapping.get(word, float(\"Inf\"))):\n",
    "                    tf_idf_title_min[document] = word_mapping.get(word)\n",
    "\n",
    "            if (tf_idf_title_max.get(document, 0) == 0):\n",
    "                tf_idf_title_max[document] = word_mapping.get(word, 0)\n",
    "            else:\n",
    "                if (tf_idf_title_max[document] < word_mapping.get(word, 0)):\n",
    "                    tf_idf_title_max[document] = word_mapping.get(word, 0)\n",
    "\n",
    "    tf_idf_title_mean = deepcopy(tf_idf_title_sum)\n",
    "    for document, tf in tf_idf_title_mean.items():\n",
    "        if (covered_terms_title[document] != 0):\n",
    "            tf_idf_title_mean[document] /= covered_terms_title[document]\n",
    "        else:\n",
    "            tf_idf_title_mean[document] = 0\n",
    "\n",
    "    rank_pages = {}\n",
    "    for document in inv_index.keys():\n",
    "        ip = []\n",
    "        ip.append(40*covered_terms_title[document])\n",
    "        ip = sum(ip)\n",
    "        rank_pages[document] = ip\n",
    "\n",
    "    page_rank_order = sorted(rank_pages.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    message = \"TOP RESULTS FOUND \\n\" \\\n",
    "          \"Enter the number of results which you need to display \\n\" \\\n",
    "          \" Press cancel to close the popup\"\n",
    "\n",
    "    i=user_input(message,\"Number of results needed\")\n",
    "    \n",
    "    results = []\n",
    "    for i in range(0, int(i)):\n",
    "        print(i+1, webLinks[page_rank_order[i][0]])\n",
    "\n",
    "    # print(results)\n",
    "    show_links_gui(webLinks,page_rank_order,query,int(i))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
